#!/usr/bin/env python
import datetime as dt
import redis as r
import uuid, ciso8601

### Begin Logging Setup ###
import logging

# Need to add our own logging formatter to enable millisecond time resolution
class MyFormatter(logging.Formatter):
    converter=dt.datetime.fromtimestamp
    def format(self, record):
        record.msg = record.msg.replace("\n","\\n").replace("\r","\\r").replace('"', '\\"')
        return super(MyFormatter, self).format(record)
    def formatTime(self, record, datefmt=None):
        ct = self.converter(record.created)
        if datefmt:
            s = ct.strftime(datefmt)
        else:
            t = ct.strftime("%Y-%m-%d %H:%M:%S")
            s = "%s.%03d" % (t, record.msecs)
        return s

# We log json to stderr, which is picked up by the service-adapter and published
# to the respective message bus.
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)
console_handler = logging.StreamHandler()
formatter = MyFormatter('{"log_level": "%(levelname)s", "created_at": "%(asctime)s", "log_message": "%(message)s"}', '%Y-%m-%dT%H:%M:%S.%fZ')
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)
### End Logging Setup ###


r = redis.StrictRedis(host='redis', port=6379, db=0)

### BASIC FUNCTIONAL PATTERN ###
#
# (Prioritization is implemented on the message- and method-level)
#
# declare constants
TICK_INTERVAL = 3000
WORKER_EXPIRY_PERIOD = TICK_INTERVAL * 2
MAX_LEASE_TIME = TICK_INTERVAL * 2
#
# JOBS_QUEUED - Redis List - holds available jobs
JOBS_QUEUED = "jobs_queued:"
#
# JOBS_DISPATCHED - Redis Sorted Set - holds jobs dispatched but not yet accepted
JOBS_DISPATCHED = "jobs_dispatched:"
#
# JOBS_FINISHED - Redis Sorted Set - holds finished jobs
JOBS_FINISHED = "jobs_finished:"
#
# JOBS_PROCESSING - Redis Sorted Set - holds dispatched jobs, which have been 
# accepted by a worker and are therefore processing now. Score is the timestamp 
# of the job_accepted message at first and the timestamp of the last 
# worker_progress message later.
JOBS_PROCESSING = "jobs_processing:"
#
# JOBS_PROGRESS - Redis Sorted Set - holds the same jobs as JOBS_PROCESSING but SCORE is the
# percentage finished represented by a float between 1 and 0.
# (this information could as well be stored in the job_uuid hash instead)
JOBS_PROGRESS = "jobs_progress:"
#
# WORKERS_LAST_SEEN - Redis Sorted Set - is there just for maintenance, it holds
# the timestamp of the last worker_idle, worker_progress, worker_finished as its
# score. On tick we purge workers older than WORKER_EXPIRY_PERIOD from both 
# WORKERS_IDLE_SET and WORKERS_BUSY_SET. 
# WARNING: 
# Workers MUST send and idle or progress message at least on every tick!!!
WORKERS_LAST_SEEN = "workers_last_seen:"
#
# WORKERS_IDLE_SET - Redis Set - holds currently idle workers. When a 
# worker_idle is received we add the worker to this group and remove it from 
# WORKERS_BUSY_SET. When a job is dispatched, we move the worker from 
# WORKERS_IDLE_SET to WORKERS_BUSY_SET.
WORKERS_IDLE_SET = "workers_idle:"
#
# WORKERS_BUSY_SET - Redis Set - holds currently busy workers. When a 
# job_accepted or a worker_progress is received, we add worker to this group and
# remove it from WORKERS_IDLE_SET. When a job_finished is received, we move 
# the worker back to the WORKERS_IDLE_SET.
WORKERS_BUSY_SET = "workers_busy:"
#
# LAST_SEEN, IDLE and BUSY are sufficient, as jobs, which have been 
# wrongfully dispatched, for whatever reason, will be readmitted to JOBS_QUEUED 
# anyways after MAX_LEASE_TIME. So we are fail-safe by design and therefore 
# immune to little hick-ups on this end, however likely or unlikeley they are. 
#
### END - BASIC FUNCTIONAL PATTERN ###


namespace_listener = os.getenv("NAMESPACE_LISTENER", "")
namespace_publisher = os.getenv("NAMESPACE_PUBLISHER", "")

# hset job_uuid job
# lpush to JOBS_QUEUED:queue_name
def addJob(message_obj):
    logger.debug("addJob")
    queue_name = message_obj["payload"]["queue_name"]
    job_uuid = message_obj["payload"]["uuid"]
    pipe = r.pipeline()
    pipe.multi()
    pipe.hset(job_uuid, "json", json.dumps(message_obj["payload"], separators=(',',':')))
    pipe.lpush(JOBS_QUEUED + queue_name, job_uuid)
    pipe.execute();
    
def workerIdle(message_obj):
    logger.debug("workerIdle")
    queue_name = message_obj["payload"]["queue_name"]
    last_seen = ciso8601.parse_datetime(message_obj["created_at"]).timestamp()
    worker_uuid = message_obj["service_uuid"]
    pipe = r.pipeline()
    pipe.multi()
    pipe.sadd(WORKERS_IDLE_SET + queue_name, worker_uuid)
    pipe.srem(WORKERS_BUSY_SET + queue_name, worker_uuid)
    pipe.zadd(WORKERS_LAST_SEEN + queue_name, last_seen, worker_uuid)
    pipe.execute()

def workerBusy(message_obj):
    logger.debug("workerBusy")
    queue_name = message_obj["payload"]["queue_name"]
    last_seen = ciso8601.parse_datetime(message_obj["created_at"]).timestamp()
    worker_uuid = message_obj["service_uuid"]
    try:
        job_progress = message_obj["payload"]["status"]["progress"]
    except:
        job_progress = 0
    pipe = r.pipeline()
    pipe.multi()
    pipe.srem(WORKERS_IDLE_SET + queue_name, worker_uuid)
    pipe.sadd(WORKERS_BUSY_SET + queue_name, worker_uuid)
    pipe.zadd(WORKERS_LAST_SEEN + queue_name, last_seen, worker_uuid)
    pipe.zadd(JOBS_PROCESSING + queue_name, last_seen, job_uuid)
    # Progress could potentially be saved in hash instead
    pipe.zadd(JOBS_PROGRESS + queue_name, job_progress, job_uuid)
    pipe.execute()

def jobFinished(message_obj):
    logger.debug("jobFinished")
    queue_name = message_obj["payload"]["queue_name"]
    job_uuid = message_obj["payload"]["uuid"]
    finished_at = message_obj["payload"]["finished_at"]
    last_seen = ciso8601.parse_datetime(message_obj["created_at"]).timestamp()
    worker_uuid = message_obj["service_uuid"]
    pipe = r.pipeline()
    pipe.multi()
    pipe.sadd(WORKERS_IDLE_SET + queue_name, worker_uuid)
    pipe.srem(WORKERS_BUSY_SET + queue_name, worker_uuid)
    pipe.zadd(WORKERS_LAST_SEEN + queue_name, last_seen, worker_uuid)
    pipe.zrem(JOBS_PROCESSING + queue_name, job_uuid)
    pipe.zrem(JOBS_PROGRESS + queue_name, job_uuid)
    pipe.hset(job_uuid, "json", json.dumps(message_obj["payload"], separators=(',',':')), "finished_at", finished_at, "worker_uuid", worker_uuid)
    pipe.zadd(JOBS_FINISHED + queue_name, finished_at, job_uuid)
    pipe.execute()

dispatchJob_lua = """
local job_uuid = redis.call("RPOP", KEYS[1])
local worker_uuid = redis.call("SRANDMEMBER", KEYS[3])

if job_uuid != nil and worker_uuid != nil then
    redis.call("ZADD", KEYS[2], tonumber(ARGV[1]), job_uuid)
    redis.call("HSET", job_uuid, "dispatched_at", tonumber(ARGV[1]))
    redis.call("SREM", worker_uuid, KEYS[3])
    redis.call("SADD", worker_uuid, KEYS[4])
    job_json = redis.call("HGET", job_uuid, "json")
    return {job_json, worker_uuid}
else
    redis.call("RPUSH", KEYS[1], job_uuid)
    return false
end
"""
dispatchJob_script = r.register_script(dispatchJob_lua)

def dispatchJob(message_obj):
    logger.debug("jobFinished")
    queue_name = message_obj["payload"]["queue_name"]
    # cleanup duplicated entries
    r.sdiffstore(WORKERS_IDLE_SET + queue_name, WORKERS_BUSY_SET + queue_name, WORKERS_IDLE_SET + queue_name)
    dispatched_at = dt.datetime.now().isoformat() + "Z"
    result = dispatchJob_script(keys=[
        JOBS_QUEUED + queue_name,
        JOBS_DISPATCHED + queue_name,
        WORKERS_IDLE_SET + queue_name,
        WORKERS_BUSY_SET + queue_name
        ], args=[dispatched_at])
    if result:
        job_json = result[0]
        worker_uuid = result[1]
        job_obj = json.loads(job_json)
        job_obj["dispatched_at"] = dispatched_at
        topic = namespace_publisher + "job_queue/workers/" + worker_uuid + "/job_assignment"
        stdoutMessage(topic, json.dumps(job_obj))
        # that should be it!
    
def jobAccepted(message_obj):
    logger.debug("jobAccepted")
    # removing job from JOBS_DISPATCHED
    # adding job to JOBS_PROCESSING with started_at as score
    # adding job to JOBS_PROGRESS with 0 as score
    queue_name = message_obj["payload"]["queue_name"]
    job_uuid = message_obj["payload"]["uuid"]
    started_at = message_obj["payload"]["started_at"]
    worker_uuid = message_obj["service_uuid"]
    pipe = r.pipeline()
    pipe.multi()
    pipe.zrem(JOBS_DISPATCHED + queue_name, job_uuid)
    pipe.zadd(JOBS_PROCESSING + queue_name, started_at, job_uuid);
    pipe.zadd(JOBS_PROGRESS + queue_name, 0, job_uuid);
    pipe.hset(job_uuid, "json", json.dumps(message_obj["payload"], separators=(',',':')), "started_at", message_obj["payload"]["started_at"], "worker_uuid", worker_uuid)
    pipe.execute();

re_add_unaccepted_jobs_lua = """
local stale_jobs = redis.call("ZREVRANGEBYSCORE", KEYS[2], tonumber(ARGV[1]), "+inf")

for index, job_uuid in ipairs(stale_jobs) do
  redis.call("ZREM", KEYS[2], job_uuid)
  redis.call("RPUSH", KEYS[1], job_uuid)
end
"""
re_add_unaccepted_jobs_script = r.register_script(re_add_unaccepted_jobs_lua)

def reAddUnacceptedJobs(message_obj):
    logger.debug("reAddUnacceptedJobs")
    # move jobs in JOBS_DISPATCHED older than MAX_LEASE_TIME back to JOBS_QUEUED
    queue_name = message_obj["payload"]["queue_name"]
    from_timestamp = dt.datetime.now().timestamp() - MAX_LEASE_TIME
    re_add_unaccepted_jobs_script(keys=[
        JOBS_QUEUED + queue_name,
        JOBS_DISPATCHED + queue_name
        ], args=[from_timestamp])

remove_stale_workers_lua = """
local stale_workers = redis.call("ZREVRANGEBYSCORE", KEYS[3], "-inf", tonumber(ARGV[1]))

for index, job_uuid in ipairs(stale_workers) do
  redis.call("SREM", KEYS[1], job_uuid)
  redis.call("SREM", KEYS[2], job_uuid)
  redis.call("ZREM", KEYS[3], job_uuid)
end
"""
remove_stale_workers_script = r.register_script(remove_stale_workers_lua)

def removeStaleWorkers(message_obj):
    logger.debug("removeStaleWorkers")
    # move workers not heard from for longer than WORKER_EXPIRY_PERIOD
    queue_name = message_obj["payload"]["queue_name"]
    from_timestamp = dt.datetime.now().timestamp() - WORKER_EXPIRY_PERIOD
    remove_stale_workers_script(keys=[
        WORKERS_IDLE_SET + queue_name,
        WORKERS_BUSY_SET + queue_name,
        WORKERS_LAST_SEEN + queue_name
        ], args=[from_timestamp])
    

# reading line by line from stdin
try:
    for json_string in sys.stdin:
        json_string = json_string.strip()
        try:
            # first we check if the payload is valid JSON
            try:
                message_obj = json.loads(json_string)
            except ValueError as e:  # includes simplejson.decoder.JSONDecodeError
                logger.error('json_parse_error: %(error)s. string_parsed: %(json_string)s' % {'error': str(e), 'json_string': json_string})
                raise
            topic = message_obj["topic"].replace(namespace_listener + "/","")
            prefix = "job_queue/"
            if topic === prefix + "add_job":
                addJob(message_obj)    
                reAddUnacceptedJobs(message_obj)
                removeStaleWorkers(message_obj)
                dispatchJob(message_obj)
            elif topic === "tick":
                reAddUnacceptedJobs(message_obj)
                removeStaleWorkers(message_obj)
            elif topic === prefix + "worker_idle":
                workerIdle(message_obj)
                # do not serve messages older than TICK_INTERVAL
                if ciso8601.parse_datetime(message_obj["created_at"]).timestamp() > dt.datetime.now().timestamp() - TICK_INTERVAL:
                    dispatchJob(message_obj)
            elif topic === prefix + "worker_busy":
                workerBusy(message_obj)
            elif topic === prefix + "job_accepted":
                jobAccepted(message_obj)
            elif topic === prefix + "job_finished":
                jobFinished(message_obj)
            elif topic === "list_queue_jobs":
            elif topic === "list_queue_workers":
            elif topic === "list_queue_progress":
            elif topic === "list_queue_dispatched":
except IOError as e:
    if e.errno == errno.EPIPE:
        logger.error("service-processor stdin PIPE was closed")
    else:
        logger.error("service-processor error")
