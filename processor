#!/usr/bin/env python
import datetime as dt
import redis
import uuid, ciso8601, os, sys, json, socket, re

### Begin Logging Setup ###
import logging

# Need to add our own logging formatter to enable millisecond time resolution
class MyFormatter(logging.Formatter):
    converter=dt.datetime.fromtimestamp
    def format(self, record):
        record.msg = record.msg.replace("\n","\\n").replace("\r","\\r").replace('"', '\\"')
        return super(MyFormatter, self).format(record)
    def formatTime(self, record, datefmt=None):
        ct = self.converter(record.created)
        if datefmt:
            s = ct.strftime(datefmt)
        else:
            t = ct.strftime("%Y-%m-%d %H:%M:%S")
            s = "%s.%03d" % (t, record.msecs)
        return s

# We log json to stderr, which is picked up by the service-adapter and published
# to the respective message bus.
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)
console_handler = logging.StreamHandler()
formatter = MyFormatter('{"log_level": "%(levelname)s", "created_at": "%(asctime)s", "log_message": "%(message)s"}', '%Y-%m-%dT%H:%M:%S.%fZ')
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)
### End Logging Setup ###

service_name = os.getenv("SERVICE_NAME", "job_queue")
service_uuid = os.getenv("SERVICE_UUID", str(uuid.uuid4()))
service_host = os.getenv("SERVICE_HOST", socket.gethostname())
namespace_listener = os.getenv("NAMESPACE_LISTENER", "default")
namespace_publisher = os.getenv("NAMESPACE_PUBLISHER", "default")


r = redis.StrictRedis(host='redis', port=6379, db=0)

### BASIC FUNCTIONAL PATTERN ###
#
# (Prioritization is implemented on the message- and method-level)
#
# declare constants
TICK_INTERVAL = 3.0 #seconds
WORKER_EXPIRY_PERIOD = TICK_INTERVAL * 3
MAX_LEASE_TIME = TICK_INTERVAL * 3
#
# JOBS_QUEUED - Redis List - holds available jobs
JOBS_QUEUED = "jobs_queued:"
#
# JOBS_DISPATCHED - Redis Sorted Set - holds jobs dispatched but not yet accepted
JOBS_DISPATCHED = "jobs_dispatched:"
#
# JOBS_FINISHED - Redis Sorted Set - holds finished jobs with finished_at as score
JOBS_FINISHED = "jobs_finished:"
#
# JOBS_PROCESSING - Redis Sorted Set - holds dispatched jobs, which have been
# accepted by a worker and are therefore processing now. Score is the timestamp
# of the job_accepted message at first and the timestamp of the last
# worker_progress message later.
JOBS_PROCESSING = "jobs_processing:"
#
# JOBS_PROGRESS - Redis Sorted Set - holds the same jobs as JOBS_PROCESSING but SCORE is the
# percentage finished represented by a float between 1 and 0.
# (this information could as well be stored in the job_uuid hash instead)
JOBS_PROGRESS = "jobs_progress:"
#
# WORKERS_LAST_SEEN - Redis Sorted Set - is there just for maintenance, it holds
# the timestamp of the last worker_idle, worker_progress, worker_finished as its
# score. On tick we purge workers older than WORKER_EXPIRY_PERIOD from both
# WORKERS_IDLE_SET and WORKERS_BUSY_SET.
# WARNING:
# Workers MUST send and idle or progress message at least on every tick!!!
WORKERS_LAST_SEEN = "workers_last_seen:"
#
# WORKERS_IDLE_SET - Redis Set - holds currently idle workers. When a
# worker_idle is received we add the worker to this group and remove it from
# WORKERS_BUSY_SET. When a job is dispatched, we move the worker from
# WORKERS_IDLE_SET to WORKERS_BUSY_SET.
WORKERS_IDLE_SET = "workers_idle:"
#
# WORKERS_BUSY_SET - Redis Set - holds currently busy workers. When a
# job_accepted or a worker_progress is received, we add worker to this group and
# remove it from WORKERS_IDLE_SET. When a job_finished is received, we move
# the worker back to the WORKERS_IDLE_SET.
WORKERS_BUSY_SET = "workers_busy:"
#
# LAST_SEEN, IDLE and BUSY are sufficient, as jobs, which have been
# wrongfully dispatched, for whatever reason, will be readmitted to JOBS_QUEUED
# anyways after MAX_LEASE_TIME. So we are fail-safe by design and therefore
# immune to little hick-ups on this end, however likely or unlikeley they are.
#
### END - BASIC FUNCTIONAL PATTERN ###

# hset job_uuid job
# lpush to JOBS_QUEUED:queue_name
def addJob(message_obj):
    logger.debug("addJob")
    queue_name = message_obj["payload"]["queue_name"]
    job_uuid = message_obj["payload"]["uuid"]
    pipe = r.pipeline()
    pipe.multi()
    pipe.hset(job_uuid, "json", json.dumps(message_obj["payload"], separators=(',',':')))
    pipe.lpush(JOBS_QUEUED + queue_name, job_uuid)
    pipe.execute();

worker_idle_lua = """
    redis.call("SADD", KEYS[1], ARGV[1])
    redis.call("SREM", KEYS[2], ARGV[1])
    redis.call("ZADD", KEYS[3], ARGV[2], ARGV[1])
    return redis.call("SMEMBERS", KEYS[1])
"""
worker_idle_script = r.register_script(worker_idle_lua)

def workerIdle(message_obj):
    logger.debug("workerIdle")
    queue_name = message_obj["payload"]["queue_name"]
    last_seen = ciso8601.parse_datetime(message_obj["created_at"]).timestamp()
    worker_uuid = message_obj["service_uuid"]
    logger.debug(queue_name + " " + str(last_seen) + " " + worker_uuid + " " + WORKERS_IDLE_SET + queue_name + " " + WORKERS_BUSY_SET + queue_name)
    result = worker_idle_script(keys=[
        WORKERS_IDLE_SET + queue_name,
        WORKERS_BUSY_SET + queue_name,
        WORKERS_LAST_SEEN + queue_name
        ], args=[worker_uuid, last_seen])
    logger.debug(str(result))

def workerBusy(message_obj):
    logger.debug("workerBusy")
    queue_name = message_obj["payload"]["queue_name"]
    last_seen = ciso8601.parse_datetime(message_obj["created_at"]).timestamp()
    worker_uuid = message_obj["service_uuid"]
    try:
        job_progress = message_obj["payload"]["status"]["progress"]
    except:
        job_progress = 0
    pipe = r.pipeline()
    pipe.multi()
    pipe.srem(WORKERS_IDLE_SET + queue_name, worker_uuid)
    pipe.sadd(WORKERS_BUSY_SET + queue_name, worker_uuid)
    pipe.zadd(WORKERS_LAST_SEEN + queue_name, last_seen, worker_uuid)
    pipe.zadd(JOBS_PROCESSING + queue_name, last_seen, job_uuid)
    # Progress could potentially be saved in hash instead
    pipe.zadd(JOBS_PROGRESS + queue_name, job_progress, job_uuid)
    pipe.execute()

def jobFinished(message_obj):
    logger.debug("jobFinished")
    queue_name = message_obj["payload"]["queue_name"]
    job_uuid = message_obj["payload"]["uuid"]
    finished_at = message_obj["payload"]["finished_at"]
    last_seen = ciso8601.parse_datetime(message_obj["created_at"]).timestamp()
    worker_uuid = message_obj["service_uuid"]
    pipe = r.pipeline()
    pipe.multi()
    pipe.sadd(WORKERS_IDLE_SET + queue_name, worker_uuid)
    pipe.srem(WORKERS_BUSY_SET + queue_name, worker_uuid)
    pipe.zadd(WORKERS_LAST_SEEN + queue_name, last_seen, worker_uuid)
    pipe.zrem(JOBS_PROCESSING + queue_name, job_uuid)
    pipe.zrem(JOBS_PROGRESS + queue_name, job_uuid)
    pipe.hset(job_uuid, "json", json.dumps(message_obj["payload"], separators=(',',':')), "finished_at", finished_at, "worker_uuid", worker_uuid)
    pipe.zadd(JOBS_FINISHED + queue_name, finished_at, job_uuid)
    pipe.execute()

dispatchJob_lua = """
redis.replicate_commands()
local number_of_jobs = redis.call("LLEN", KEYS[1])
local number_of_workers = redis.call("SCARD", KEYS[3])

if (number_of_jobs > 0 and number_of_workers > 0) then
    local job_uuid = redis.call("RPOP", KEYS[1])
    local worker_uuid = redis.call("SPOP", KEYS[3])
    redis.call("SADD", KEYS[4], worker_uuid)
    redis.call("ZADD", KEYS[2], ARGV[2], job_uuid )
    redis.call("HSET", job_uuid, "dispatched_at", ARGV[1])
    local job_json = redis.call("HGET", job_uuid, "json")
    return {job_json, worker_uuid}
else
    return false
end
"""
dispatchJob_script = r.register_script(dispatchJob_lua)

def dispatchJob(message_obj):
    logger.debug("dispatchJob")
    queue_name = message_obj["payload"]["queue_name"]
    # cleanup duplicated entries
    #r.sdiffstore(WORKERS_IDLE_SET + queue_name, WORKERS_BUSY_SET + queue_name, WORKERS_IDLE_SET + queue_name)
    now = dt.datetime.now()
    dispatched_at = now.isoformat() + "Z"
    dispatched_ts = now.timestamp()
    logger.debug("dispatched_at: " + dispatched_at + ", dispatched_ts: " + str(dispatched_ts))
    result = dispatchJob_script(keys=[
        JOBS_QUEUED + queue_name,
        JOBS_DISPATCHED + queue_name,
        WORKERS_IDLE_SET + queue_name,
        WORKERS_BUSY_SET + queue_name
        ], args=[dispatched_at, dispatched_ts])
    logger.debug("result: " + str(result))
    if result:
        logger.debug("result available")
        logger.debug("result: " + result[0].decode("utf-8"))
        logger.debug("result: " + result[1].decode("utf-8"))
        job_json = result[0].decode("utf-8")
        worker_uuid = result[1].decode("utf-8")
        job_obj = json.loads(job_json)
        job_obj["dispatched_at"] = dispatched_at
        topic = namespace_publisher + "/job_queue/workers/" + worker_uuid + "/job_assignment"
        stdoutMessage(topic, job_obj)

def jobAccepted(message_obj):
    logger.debug("jobAccepted")
    # removing job from JOBS_DISPATCHED
    # adding job to JOBS_PROCESSING with started_at as score
    # adding job to JOBS_PROGRESS with 0 as score
    queue_name = message_obj["payload"]["queue_name"]
    job_uuid = message_obj["payload"]["uuid"]
    started_at = ciso8601.parse_datetime(message_obj["payload"]["started_at"]).timestamp()
    worker_uuid = message_obj["service_uuid"]
    pipe = r.pipeline()
    pipe.multi()
    pipe.zrem(JOBS_DISPATCHED + queue_name, job_uuid)
    pipe.zadd(JOBS_PROCESSING + queue_name, started_at, job_uuid);
    pipe.zadd(JOBS_PROGRESS + queue_name, 0, job_uuid);
    pipe.hmset(job_uuid, {"json":json.dumps(message_obj["payload"], separators=(',',':')), "started_at":message_obj["payload"]["started_at"], "worker_uuid": worker_uuid})
    pipe.execute();

re_add_unaccepted_jobs_lua = """
local stale_jobs = redis.call("ZREVRANGEBYSCORE", KEYS[2], tonumber(ARGV[1]), "-inf")

for index, job_uuid in ipairs(stale_jobs) do
  redis.call("ZREM", KEYS[2], job_uuid)
  redis.call("RPUSH", KEYS[1], job_uuid)
end
"""
re_add_unaccepted_jobs_script = r.register_script(re_add_unaccepted_jobs_lua)

def reAddUnacceptedJobs(message_obj):
    logger.debug("reAddUnacceptedJobs")
    # move jobs in JOBS_DISPATCHED older than MAX_LEASE_TIME back to JOBS_QUEUED
    queue_name = message_obj["payload"]["queue_name"]
    from_timestamp = dt.datetime.now().timestamp() - MAX_LEASE_TIME
    re_add_unaccepted_jobs_script(keys=[
        JOBS_QUEUED + queue_name,
        JOBS_DISPATCHED + queue_name
        ], args=[from_timestamp])

remove_stale_workers_lua = """
local stale_workers = redis.call("ZREVRANGEBYSCORE", KEYS[3], tonumber(ARGV[1]), "-inf")

for index, job_uuid in ipairs(stale_workers) do
  redis.call("SREM", KEYS[1], job_uuid)
  redis.call("SREM", KEYS[2], job_uuid)
  redis.call("ZREM", KEYS[3], job_uuid)
end
"""
remove_stale_workers_script = r.register_script(remove_stale_workers_lua)

def removeStaleWorkers(message_obj):
    logger.debug("removeStaleWorkers")
    # move workers not heard from for longer than WORKER_EXPIRY_PERIOD
    queue_name = message_obj["payload"]["queue_name"]
    from_timestamp = dt.datetime.now().timestamp() - WORKER_EXPIRY_PERIOD
    remove_stale_workers_script(keys=[
        WORKERS_IDLE_SET + queue_name,
        WORKERS_BUSY_SET + queue_name,
        WORKERS_LAST_SEEN + queue_name
        ], args=[from_timestamp])

def stdoutMessage(topic, payload):
    message_obj = {}
    message_obj["topic"] = topic
    message_obj["service_name"] = service_name
    message_obj["service_uuid"] = service_uuid
    message_obj["service_host"] = service_host
    # Generates datetime ISO 8601 format ex. 2017-04-24T21:16:26.678Z
    message_obj["created_at"] = dt.datetime.now().isoformat() + "Z"
    message_obj["payload"] = payload

    print(json.dumps(message_obj, separators=(',',':')))
    sys.stdout.flush()


# reading line by line from stdin
# FIXME: main keys need initialization, if they are not available yet
try:
    for json_string in sys.stdin:
        json_string = json_string.strip()
        # first we check if the payload is valid JSON
        try:
            message_obj = json.loads(json_string)
        except ValueError as e:  # includes simplejson.decoder.JSONDecodeError
            logger.error('json_parse_error: %(error)s. string_parsed: %(json_string)s' % {'error': str(e), 'json_string': json_string})
            raise
        try:
            topic = message_obj["topic"].replace(namespace_listener + "/","")
            prefix = "job_queue/"
            if topic == prefix + "add_job":
                addJob(message_obj)
                reAddUnacceptedJobs(message_obj)
                removeStaleWorkers(message_obj)
                dispatchJob(message_obj)
            elif topic == "tick":
                pass
            elif re.search('^job_queue/workers/\w{8}-\w{4}-\w{4}-\w{4}-\w{12}/(worker_idle)$', topic):
                logger.debug("worker_idle")
                workerIdle(message_obj)
                reAddUnacceptedJobs(message_obj)
                removeStaleWorkers(message_obj)
                # do not serve messages older than TICK_INTERVAL
                if ciso8601.parse_datetime(message_obj["created_at"]).timestamp() > dt.datetime.now().timestamp() - TICK_INTERVAL:
                    dispatchJob(message_obj)
            elif topic == prefix + "worker_busy":
                workerBusy(message_obj)
            elif re.search('^job_queue/workers/\w{8}-\w{4}-\w{4}-\w{4}-\w{12}/(job_accepted)$', topic):
                jobAccepted(message_obj)
            elif re.search('^job_queue/workers/\w{8}-\w{4}-\w{4}-\w{4}-\w{12}/(job_finished)$', topic):
                jobFinished(message_obj)
            elif topic == "list_queue_jobs":
                pass
            elif topic == "list_queue_workers":
                pass
            elif topic == "list_queue_progress":
                pass
            elif topic == "list_queue_dispatched":
                pass
            elif topic == "del_job":
                    pass
        except KeyError as e:
            logger.error('KeyError: %(error)s. string_parsed: %(json_string)s' % {'error': str(e), 'json_string': json_string})
except IOError as e:
    if e.errno == errno.EPIPE:
        logger.error("service-processor stdin PIPE was closed")
    else:
        logger.error("service-processor error")
